{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Produce Data\n",
    "We elected to use the datagen connector to generate fake data for this assignment. The topic we used was 'stocktrades. The steps were as follows:\n",
    "*  Open a browser and go to http://localhost:9021/\n",
    "*  Select the available cluster\n",
    "*  On the menu bar, select Connect\n",
    "*  Click on the connect-default cluster in the Connect Clusters list.\n",
    "*  Click on Add connector\n",
    "*  Select DatagenConnector\n",
    "*  Enter connector_stock_trades in the Name field\n",
    "\n",
    "Then:\n",
    "Generate a data stream with following configurations:\n",
    "```\n",
    "{\n",
    "  \"name\": \"connector_stock_trades\",\n",
    "  \"connector.class\": \"io.confluent.kafka.connect.datagen.DatagenConnector\",\n",
    "  \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n",
    "  \"kafka.topic\": \"stocktrades\",\n",
    "  \"max.interval\": \"100\",\n",
    "  \"quickstart\": \"Stock_Trades\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Using Ksql to create at least 2 streams with filtering from topics\n",
    "\n",
    "To begin, you need to create a stream called stocktrades with no filtering in place."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE STREAM STOCKTRADES\n",
    "   (SIDE STRING, QUANTITY INTEGER, SYMBOL STRING, PRICE INTEGER, ACCOUNT STRING, USERID STRING)\n",
    "       WITH (KAFKA_TOPIC='stocktrades', VALUE_FORMAT='AVRO');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create JSON Stream\n",
    "In order for this to play nicely with spark, we need to mimic the raw stream as a json formatted stream."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE STREAM STOCKTRADES_JSON WITH (KAFKA_TOPIC='STOCKTRADES_JSON', VALUE_FORMAT='JSON') AS SELECT\n",
    "* FROM STOCKTRADES STOCKTRADES\n",
    "EMIT CHANGES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream 1 - Sell Stream\n",
    "It may be in the interest of the business to view only streams where the stock was sold and not bought. This would be useful in identifying which shares should be taken as a 'short' position"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE STREAM SELL_TRADES WITH (KAFKA_TOPIC='SELL_TRADES', VALUE_FORMAT='JSON') AS SELECT\n",
    "  STOCKTRADES_JSON.QUANTITY QUANTITY,\n",
    "  STOCKTRADES_JSON.SYMBOL SYMBOL,\n",
    "  STOCKTRADES_JSON.PRICE PRICE,\n",
    "  STOCKTRADES_JSON.ACCOUNT ACCOUNT,\n",
    "  STOCKTRADES_JSON.USERID USERID\n",
    "FROM STOCKTRADES_JSON STOCKTRADES_JSON\n",
    "WHERE (STOCKTRADES_JSON.SIDE = 'SELL')\n",
    "EMIT CHANGES;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream 2 - Buy Stream\n",
    "It may also be interesting to the business to see trades that were large buys."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE STREAM BUY_TRADES WITH (KAFKA_TOPIC='BUY_TRADES', VALUE_FORMAT='JSON') AS SELECT\n",
    "  STOCKTRADES_JSON.QUANTITY QUANTITY,\n",
    "  STOCKTRADES_JSON.SYMBOL SYMBOL,\n",
    "  STOCKTRADES_JSON.PRICE PRICE,\n",
    "  STOCKTRADES_JSON.ACCOUNT ACCOUNT,\n",
    "  STOCKTRADES_JSON.USERID USERID\n",
    "FROM STOCKTRADES_JSON STOCKTRADES_JSON\n",
    "WHERE (STOCKTRADES_JSON.SIDE = 'BUY')\n",
    "EMIT CHANGES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 - Aggregated Buy Trades\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE AGG_BUY_ORDERS WITH (KAFKA_TOPIC='AGG_BUY_ORDERS', VALUE_FORMAT='JSON') AS SELECT\n",
    "  BUY_TRADES.SYMBOL SYMBOL,\n",
    "  SUM(BUY_TRADES.QUANTITY) QUANTITY_AGG,\n",
    "  AVG(BUY_TRADES.PRICE) PRICE_AVG,\n",
    "  SUM((BUY_TRADES.QUANTITY * BUY_TRADES.PRICE)) VALUE_TRADED\n",
    "FROM BUY_TRADES BUY_TRADES\n",
    "WINDOW TUMBLING ( SIZE 60 SECONDS )\n",
    "GROUP BY BUY_TRADES.SYMBOL\n",
    "EMIT CHANGES;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2 - Aggregated Sell Trades"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "CREATE TABLE AGG_SELL_ORDERS WITH (KAFKA_TOPIC='AGG_SELL_ORDERS', VALUE_FORMAT='JSON') AS SELECT\n",
    "  SELL_TRADES.SYMBOL SYMBOL,\n",
    "  SUM(SELL_TRADES.QUANTITY) QUANTITY_AGG,\n",
    "  AVG(SELL_TRADES.PRICE) PRICE_AVG,\n",
    "  SUM((SELL_TRADES.QUANTITY * SELL_TRADES.PRICE)) VALUE_TRADED\n",
    "FROM SELL_TRADES SELL_TRADES\n",
    "WINDOW TUMBLING ( SIZE 60 SECONDS )\n",
    "GROUP BY SELL_TRADES.SYMBOL\n",
    "EMIT CHANGES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3 - Consume/Transform data with Spark Streaming\n",
    "\n",
    "### Set up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StringType, StructField, IntegerType, FloatType, BinaryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('kafka') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building a Stream\n",
    "The code below just walks through how the stream is built. A streamlined function is built for part 3a below.\n",
    "\n",
    "Firstly, we need to connect to our raw JSON stream 'STOCKTRADES_JSON'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"STOCKTRADES_JSON\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a stream to view the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stream = stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"raw_stocktrades_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT key, value FROM raw_stocktrades_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the key being NULL, the key is equivalent to the `value.symbol` field as shown in confluent. We are happy to proceed without this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert Key Value pairs to strings\n",
    "Converting to strings allows us to read the actual content of the key and value. Above the values are in hexadecimal binary, which doesnt make sesne to a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_stream_df = stream_df \\\n",
    "    .withColumn(\"key\", stream_df[\"key\"].cast(StringType())) \\\n",
    "      .withColumn('value', stream_df[\"value\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_stream = string_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"string_stocktrades_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| key|               value|\n",
      "+----+--------------------+\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"SELL\",\"Q...|\n",
      "|null|{\"SIDE\":\"BUY\",\"QU...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT key, value FROM string_stocktrades_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformation\n",
    "\n",
    "For future parts, we will need the data in a tidy format, not a JSON format. To do this, we need to outline the JSON structure of the data. and use the inbuilt `from_json` function from Spark to read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_stocktrades =  StructType([\n",
    "        StructField(\"SIDE\", StringType(),  True),\n",
    "        StructField(\"QUANTITY\", IntegerType(),  True),\n",
    "        StructField(\"PRICE\", IntegerType(),  True),\n",
    "        StructField(\"SYMBOL\", StringType(),  True),\n",
    "        StructField(\"ACCOUNT\", StringType(), True),\n",
    "         StructField(\"USERID\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream_df = string_stream_df\\\n",
    "    .withColumn(\"value\", F.from_json(\"value\", schema_stocktrades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- SIDE: string (nullable = true)\n",
      " |    |-- QUANTITY: integer (nullable = true)\n",
      " |    |-- PRICE: integer (nullable = true)\n",
      " |    |-- SYMBOL: string (nullable = true)\n",
      " |    |-- ACCOUNT: string (nullable = true)\n",
      " |    |-- USERID: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream = json_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"extract_stocktrades_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "|key |value                                   |topic           |partition|offset|timestamp              |timestampType|\n",
      "+----+----------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "|null|{BUY, 1934, 893, ZTEST, ABC123, User_8} |STOCKTRADES_JSON|0        |0     |2021-06-11 00:40:34.802|0            |\n",
      "|null|{SELL, 1401, 379, ZVV, ABC123, User_4}  |STOCKTRADES_JSON|0        |1     |2021-06-11 00:40:34.837|0            |\n",
      "|null|{SELL, 2310, 848, ZJZZT, LMN456, User_6}|STOCKTRADES_JSON|0        |2     |2021-06-11 00:40:34.897|0            |\n",
      "|null|{BUY, 35, 845, ZWZZT, LMN456, User_2}   |STOCKTRADES_JSON|0        |3     |2021-06-11 00:40:34.989|0            |\n",
      "|null|{BUY, 3883, 503, ZBZX, ABC123, User_8}  |STOCKTRADES_JSON|0        |4     |2021-06-11 00:40:35.077|0            |\n",
      "|null|{SELL, 2137, 88, ZWZZT, ABC123, User_5} |STOCKTRADES_JSON|0        |5     |2021-06-11 00:40:35.108|0            |\n",
      "|null|{BUY, 3459, 21, ZBZX, XYZ789, User_6}   |STOCKTRADES_JSON|0        |6     |2021-06-11 00:40:35.138|0            |\n",
      "|null|{BUY, 828, 253, ZBZX, XYZ789, User_5}   |STOCKTRADES_JSON|0        |7     |2021-06-11 00:40:35.206|0            |\n",
      "|null|{SELL, 3058, 791, ZVV, LMN456, User_7}  |STOCKTRADES_JSON|0        |8     |2021-06-11 00:40:35.293|0            |\n",
      "|null|{SELL, 937, 399, ZXZZT, ABC123, User_6} |STOCKTRADES_JSON|0        |9     |2021-06-11 00:40:35.346|0            |\n",
      "|null|{SELL, 1856, 348, ZBZX, LMN456, User_9} |STOCKTRADES_JSON|0        |10    |2021-06-11 00:40:35.374|0            |\n",
      "|null|{SELL, 3680, 168, ZBZX, XYZ789, User_1} |STOCKTRADES_JSON|0        |11    |2021-06-11 00:40:35.459|0            |\n",
      "|null|{SELL, 3488, 920, ZTEST, LMN456, User_6}|STOCKTRADES_JSON|0        |12    |2021-06-11 00:40:35.473|0            |\n",
      "|null|{SELL, 1134, 690, ZBZX, XYZ789, User_9} |STOCKTRADES_JSON|0        |13    |2021-06-11 00:40:35.495|0            |\n",
      "|null|{BUY, 4767, 657, ZWZZT, ABC123, User_9} |STOCKTRADES_JSON|0        |14    |2021-06-11 00:40:35.565|0            |\n",
      "|null|{BUY, 2685, 662, ZWZZT, XYZ789, User_2} |STOCKTRADES_JSON|0        |15    |2021-06-11 00:40:35.57 |0            |\n",
      "|null|{SELL, 4504, 822, ZVZZT, XYZ789, User_7}|STOCKTRADES_JSON|0        |16    |2021-06-11 00:40:35.617|0            |\n",
      "|null|{SELL, 1806, 194, ZTEST, ABC123, User_3}|STOCKTRADES_JSON|0        |17    |2021-06-11 00:40:35.638|0            |\n",
      "|null|{SELL, 4687, 882, ZJZZT, LMN456, User_9}|STOCKTRADES_JSON|0        |18    |2021-06-11 00:40:35.735|0            |\n",
      "|null|{BUY, 4561, 428, ZJZZT, XYZ789, User_9} |STOCKTRADES_JSON|0        |19    |2021-06-11 00:40:35.789|0            |\n",
      "+----+----------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM extract_stocktrades_view').show(20, False))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Flatten Data\n",
    "Finally, we need to separate the data that is in the value field into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrades_stream_df = json_stream_df \\\n",
    "    .select( \\\n",
    "        F.col(\"key\").alias(\"event_key\"), \\\n",
    "        F.col(\"topic\").alias(\"event_topic\"), \\\n",
    "        F.col(\"timestamp\").alias(\"event_timestamp\"), \\\n",
    "        \"value.side\", \\\n",
    "        \"value.quantity\", \\\n",
    "        \"value.price\", \\\n",
    "        \"value.symbol\", \\\n",
    "        \"value.account\", \\\n",
    "        \"value.userid\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_key: string (nullable = true)\n",
      " |-- event_topic: string (nullable = true)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- userid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocktrades_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrades_stream = stocktrades_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"stocktrades_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|event_key|     event_topic|     event_timestamp|side|quantity|price|symbol|account|userid|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    1934|  893| ZTEST| ABC123|User_8|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    1401|  379|   ZVV| ABC123|User_4|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    2310|  848| ZJZZT| LMN456|User_6|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|      35|  845| ZWZZT| LMN456|User_2|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    3883|  503|  ZBZX| ABC123|User_8|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    2137|   88| ZWZZT| ABC123|User_5|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    3459|   21|  ZBZX| XYZ789|User_6|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|     828|  253|  ZBZX| XYZ789|User_5|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    3058|  791|   ZVV| LMN456|User_7|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|     937|  399| ZXZZT| ABC123|User_6|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    1856|  348|  ZBZX| LMN456|User_9|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    3680|  168|  ZBZX| XYZ789|User_1|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    3488|  920| ZTEST| LMN456|User_6|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    1134|  690|  ZBZX| XYZ789|User_9|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    4767|  657| ZWZZT| ABC123|User_9|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    2685|  662| ZWZZT| XYZ789|User_2|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    4504|  822| ZVZZT| XYZ789|User_7|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    1806|  194| ZTEST| ABC123|User_3|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...|SELL|    4687|  882| ZJZZT| LMN456|User_9|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 00:40:...| BUY|    4561|  428| ZJZZT| XYZ789|User_9|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM stocktrades_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrades_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3a - Build at least 2 Spark Streaming dataframes\n",
    "This function generates a stream from stocktrades with one line of code so its easier to call in later components. It is equivalent to the structure outlined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_stocktrades_stream(keep_stream = False):\n",
    "    \n",
    "    # Define the raw Spark Stream\n",
    "    stream_df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .option(\"subscribe\", \"STOCKTRADES_JSON\") \\\n",
    "      .load()\n",
    "    \n",
    "    # Convert to string types for JSON conversion\n",
    "    string_stream_df = stream_df \\\n",
    "        .withColumn(\"key\", stream_df[\"key\"].cast(StringType())) \\\n",
    "        .withColumn('value', stream_df[\"value\"].cast(StringType()))\n",
    "    \n",
    "    # Define the Schema for the end JSON format\n",
    "    schema_stocktrades =  StructType([\n",
    "        StructField(\"SIDE\", StringType(),  True),\n",
    "        StructField(\"QUANTITY\", IntegerType(),  True),\n",
    "        StructField(\"PRICE\", IntegerType(),  True),\n",
    "        StructField(\"SYMBOL\", StringType(),  True),\n",
    "        StructField(\"ACCOUNT\", StringType(), True),\n",
    "        StructField(\"USERID\", StringType(), True)\n",
    "])\n",
    "    # Convert the string type to json format stream\n",
    "    json_stream_df = string_stream_df\\\n",
    "    .withColumn(\"value\", F.from_json(\"value\", schema_stocktrades))\n",
    "    stocktrades_stream_df = json_stream_df \\\n",
    "    .select( \\\n",
    "        F.col(\"key\").alias(\"event_key\"), \\\n",
    "        F.col(\"topic\").alias(\"event_topic\"), \\\n",
    "        F.col(\"timestamp\").alias(\"event_timestamp\"), \\\n",
    "        \"value.side\", \\\n",
    "        \"value.quantity\", \\\n",
    "        \"value.price\", \\\n",
    "        \"value.symbol\", \\\n",
    "        \"value.account\", \\\n",
    "        \"value.userid\"\n",
    "    )\n",
    "    \n",
    "    # Export a queryable view od the stream\n",
    "    \n",
    "    if not keep_stream:\n",
    "        return stocktrades_stream_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"stocktrades_view\") \\\n",
    "        .start()\n",
    "    else:\n",
    "        return stocktrades_stream_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"stocktrades_view\") \\\n",
    "        .start(), stocktrades_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrades_stream = generate_stocktrades_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|event_key|     event_topic|     event_timestamp|side|quantity|price|symbol|account|userid|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:09:...| BUY|    3765|  589| ZJZZT| ABC123|User_7|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:09:...|SELL|    1090|  207| ZVZZT| ABC123|User_5|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:09:...|SELL|    1387|  942| ZTEST| ABC123|User_4|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:09:...|SELL|    2727|  902| ZWZZT| LMN456|User_6|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM stocktrades_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrades_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to stream filtered streams from Kafka as streaming dataframes\n",
    "def generate_side_stream(SIDE):\n",
    "    '''\n",
    "    This function generates a stream for the BUY and SELL topics.\n",
    "    \n",
    "    @param SIDE: 'BUY' OR 'SELL' streams.\n",
    "    \n",
    "    Returns:\n",
    "    Spark DataFrame\n",
    "    '''\n",
    "    # Create string for source\n",
    "    source_stream = SIDE + \"_TRADES\"\n",
    "    # Define the raw Spark Stream\n",
    "    \n",
    "    stream_df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .option(\"subscribe\", source_stream) \\\n",
    "      .load()\n",
    "    \n",
    "    # Convert to string types for JSON conversion\n",
    "    string_stream_df = stream_df \\\n",
    "        .withColumn(\"key\", stream_df[\"key\"].cast(StringType())) \\\n",
    "        .withColumn('value', stream_df[\"value\"].cast(StringType()))\n",
    "    \n",
    "    # Define the Schema for the end JSON format\n",
    "    schema_stocktrades =  StructType([\n",
    "        StructField(\"QUANTITY\", IntegerType(),  True),\n",
    "        StructField(\"PRICE\", IntegerType(),  True),\n",
    "        StructField(\"SYMBOL\", StringType(),  True),\n",
    "        StructField(\"ACCOUNT\", StringType(), True),\n",
    "        StructField(\"USERID\", StringType(), True)\n",
    "])\n",
    "    # Convert the string type to json format stream\n",
    "    json_stream_df = string_stream_df\\\n",
    "    .withColumn(\"value\", F.from_json(\"value\", schema_stocktrades))\n",
    "    stocktrades_stream_df = json_stream_df \\\n",
    "    .select( \\\n",
    "        F.col(\"key\").alias(\"event_key\"), \\\n",
    "        F.col(\"topic\").alias(\"event_topic\"), \\\n",
    "        F.col(\"timestamp\").alias(\"event_timestamp\"), \\\n",
    "        \"value.quantity\", \\\n",
    "        \"value.price\", \\\n",
    "        \"value.symbol\", \\\n",
    "        \"value.account\", \\\n",
    "        \"value.userid\"\n",
    "    )\n",
    "    \n",
    "    # Export a queryable view od the stream\n",
    "    return stocktrades_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(SIDE + '_view') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create views of the buy and sell streams\n",
    "BUY_stream = generate_side_stream('BUY')\n",
    "SELL_stream = generate_side_stream('SELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "|event_key|event_topic|     event_timestamp|quantity|price|symbol|account|userid|\n",
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    2366|  269| ZJZZT| ABC123|User_6|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    2677|  997| ZJZZT| LMN456|User_7|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    1417|  236| ZTEST| LMN456|User_5|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    4237|  906|  ZBZX| ABC123|User_9|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    1884|  739| ZWZZT| LMN456|User_7|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    2545|  576| ZVZZT| XYZ789|User_2|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    1073|  772| ZTEST| LMN456|User_3|\n",
      "|     null| BUY_TRADES|2021-06-11 07:09:...|    4572|  993|  ZBZX| ABC123|User_1|\n",
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results (limit 20) of stream\n",
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM BUY_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "|event_key|event_topic|     event_timestamp|quantity|price|symbol|account|userid|\n",
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    1116|  578| ZTEST| ABC123|User_1|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    2942|  172| ZVZZT| LMN456|User_7|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4191|  254| ZVZZT| ABC123|User_1|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    2499|  802| ZWZZT| LMN456|User_5|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    3006|  280| ZWZZT| LMN456|User_3|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|     236|  429| ZVZZT| XYZ789|User_6|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    3329|  270|  ZBZX| ABC123|User_6|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    3458|  871| ZVZZT| ABC123|User_6|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    2769|  912|   ZVV| LMN456|User_2|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    3481|  111| ZJZZT| XYZ789|User_9|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    1046|  243| ZXZZT| ABC123|User_3|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    3801|  595| ZVZZT| XYZ789|User_3|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    1711|  632| ZVZZT| ABC123|User_9|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    2665|  318| ZXZZT| ABC123|User_1|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4177|  847| ZTEST| LMN456|User_5|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    2256|  305| ZWZZT| ABC123|User_7|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4572|  827|  ZBZX| LMN456|User_9|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4460|  632| ZWZZT| XYZ789|User_2|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4273|  675| ZXZZT| XYZ789|User_3|\n",
      "|     null|SELL_TRADES|2021-06-11 07:09:...|    4690|  711| ZVZZT| XYZ789|User_2|\n",
      "+---------+-----------+--------------------+--------+-----+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results (limit 20) of stream\n",
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM SELL_view').show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the two streams\n",
    "BUY_stream.stop()\n",
    "SELL_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3b - Build 1 window stream with a watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the raw data data frame\n",
    "raw_stream, raw_stream_df = generate_stocktrades_stream(True)\n",
    "\n",
    "# Create parameters for the window stream\n",
    "window_duration = '60 seconds'\n",
    "slide_duration = '10 seconds'\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Windowed Stream\n",
    "this groups by symbol and then counts the numbers of trades in the window, quantity of shares traded and the average price traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_agg_df = raw_stream_df \\\n",
    "    .withWatermark('event_timestamp', '1 minutes') \\\n",
    "    .groupBy(F.window(raw_stream_df.event_timestamp, window_duration, slide_duration), raw_stream_df.symbol) \\\n",
    "    .agg(F.count('SYMBOL').alias('no_trades'), \\\n",
    "    F.sum('QUANTITY').alias('tot_quantity'), \\\n",
    "    F.avg('PRICE').alias('avg_price') \\\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the stream as a qeuryable view\n",
    "windowed_agg_stream = windowed_agg_df \\\n",
    "                        .writeStream \\\n",
    "                        .format(\"memory\") \\\n",
    "                        .outputMode(\"Complete\") \\\n",
    "                        .queryName(\"windowed_view\") \\\n",
    "                        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+------------+------------------+\n",
      "|              window|symbol|no_trades|tot_quantity|         avg_price|\n",
      "+--------------------+------+---------+------------+------------------+\n",
      "|{2021-06-11 07:09...| ZVZZT|       37|       96950| 524.2162162162163|\n",
      "|{2021-06-11 07:08...|  ZBZX|       10|       25396|             492.2|\n",
      "|{2021-06-11 07:09...|  ZBZX|       38|       99824|482.07894736842104|\n",
      "|{2021-06-11 07:08...|   ZVV|       16|       37495|          522.5625|\n",
      "|{2021-06-11 07:09...| ZTEST|       42|       93913| 541.3809523809524|\n",
      "|{2021-06-11 07:09...| ZXZZT|       51|      142049| 440.2549019607843|\n",
      "|{2021-06-11 07:09...| ZWZZT|       38|      100884| 549.7631578947369|\n",
      "|{2021-06-11 07:09...|  ZBZX|       39|      103859| 477.7435897435897|\n",
      "|{2021-06-11 07:09...| ZWZZT|       36|       95099| 537.5833333333334|\n",
      "|{2021-06-11 07:09...| ZWZZT|       38|      100884| 549.7631578947369|\n",
      "|{2021-06-11 07:09...| ZTEST|       42|       93913| 541.3809523809524|\n",
      "|{2021-06-11 07:09...|   ZVV|       46|      125861|476.30434782608694|\n",
      "|{2021-06-11 07:10...|   ZVV|        1|        4828|             568.0|\n",
      "|{2021-06-11 07:09...| ZJZZT|       46|      113652|             499.0|\n",
      "|{2021-06-11 07:08...| ZXZZT|       18|       50550|408.27777777777777|\n",
      "|{2021-06-11 07:10...| ZXZZT|        4|       12572|             204.5|\n",
      "|{2021-06-11 07:09...|   ZVV|       47|      130689|478.25531914893617|\n",
      "|{2021-06-11 07:09...| ZVZZT|       37|       96950| 524.2162162162163|\n",
      "|{2021-06-11 07:09...| ZXZZT|       51|      142049| 440.2549019607843|\n",
      "|{2021-06-11 07:09...| ZVZZT|       37|       96950| 524.2162162162163|\n",
      "+--------------------+------+---------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql('SELECT * FROM windowed_view').show())\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_agg_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3c - Build at least 1 spark query\n",
    "A number of other spark queries have been built above. For the flow, I have included a new spark query here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df = generate_stocktrades_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from stocktrades_view WHERE price > 800\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|event_key|     event_topic|     event_timestamp|side|quantity|price|symbol|account|userid|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "|     null|STOCKTRADES_JSON| 2021-06-11 07:10:19| BUY|    3239|  976|   ZVV| LMN456|User_1|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...| BUY|    1703|  963|   ZVV| LMN456|User_6|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|    3653|  801|  ZBZX| LMN456|User_3|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...| BUY|    1273|  813|  ZBZX| LMN456|User_4|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...| BUY|    3885|  988| ZWZZT| LMN456|User_8|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|    1337|  972|  ZBZX| XYZ789|User_4|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|    2297|  832| ZWZZT| ABC123|User_3|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...| BUY|    1070|  985|   ZVV| LMN456|User_9|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|     159|  938|  ZBZX| XYZ789|User_2|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|    1323|  917| ZJZZT| LMN456|User_5|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|     645|  972| ZWZZT| ABC123|User_7|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...| BUY|     744|  956| ZTEST| LMN456|User_8|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|     537|  984|  ZBZX| ABC123|User_4|\n",
      "|     null|STOCKTRADES_JSON|2021-06-11 07:10:...|SELL|    1932|  873| ZTEST| ABC123|User_8|\n",
      "+---------+----------------+--------------------+----+--------+-----+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results (limit 20) of stream\n",
    "clear_output(wait=True)\n",
    "display(spark.sql(query).show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3d - Export your spark queries into parquets stocktrade_stream\n",
    "Create the stream and write into the `parquet_output` folder in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df, raw_stream_df = generate_stocktrades_stream(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3cf0249bb0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stream_df.writeStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .option(\"path\", \"../data/parquet_output\") \\\n",
    "                .option(\"checkpointLocation\", \"checkpoint/data\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(once=True) \\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3e - build at least 1 visualisation which will be refreshed at regular interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "%matplotlib inline\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(x = 10, sleep = 5):\n",
    "    '''\n",
    "    Function to create and update function as data streams in. Note that this function is set to update 10 times\n",
    "    Parameters:\n",
    "    @param x: Number of iterations for the chart to update\n",
    "    @param sleeep: Seconds between chart updates\n",
    "    \n",
    "    '''\n",
    "    stocktrades_stream_df= generate_stocktrades_stream()\n",
    "    count = 0\n",
    "    while count <= x:\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        top_10_users = spark.sql('Select userid,sum(quantity) as quantity from stocktrades_view group by userid order by quantity desc limit 10' \n",
    "    )\n",
    "        top_10_df = top_10_users.toPandas()\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure( figsize = ( 10, 8 ) )\n",
    "        sns.barplot( x=\"quantity\", y=\"userid\", data=top_10_df)\n",
    "        plt.show()\n",
    "        count = count + 1\n",
    "        print(f'Count:{count}')\n",
    "    stocktrades_stream_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:11\n"
     ]
    }
   ],
   "source": [
    "visualise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Build ML model\n",
    "We believe it would be useful to predict whether a trade was a buy or sell trade based on the quantity, price account and stock code. The following code follows a similar pattern to that outlined in assignment 1.\n",
    "\n",
    "### Read Data from Parquet Sink\n",
    "Note that when you read in the parquet file you need to set the option `mergedSchema` to `true` this is because the parquet sink is actually a collection of parquet files, not a singular parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.option(\"mergeSchema\", \"true\").parquet(\"../data/parquet_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+------+------------------+------------------+------+-------+------+\n",
      "|summary|event_key|     event_topic|  side|          quantity|             price|symbol|account|userid|\n",
      "+-------+---------+----------------+------+------------------+------------------+------+-------+------+\n",
      "|  count|        0|          215453|215453|            215453|            215453|215453| 215453|215453|\n",
      "|   mean|     null|            null|  null| 2501.367165924819| 501.9253851187962|  null|   null|  null|\n",
      "| stddev|     null|            null|  null|1443.2353946422181|287.13652806540324|  null|   null|  null|\n",
      "|    min|     null|STOCKTRADES_JSON|   BUY|                 1|                 5|  ZBZX| ABC123|User_1|\n",
      "|    max|     null|STOCKTRADES_JSON|  SELL|              4999|               999| ZXZZT| XYZ789|User_9|\n",
      "+-------+---------+----------------+------+------------------+------------------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['symbol','account','userid']\n",
    "cols = [\"side\",'quantity','price','symbol','account','userid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parquetFile.select(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data pipeline\n",
    "In this section, we will encode categorical variables and assemble them using the vector assembler into a pipeline for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "for cat_col in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_ind\")\n",
    "    col_encoder = OneHotEncoder(inputCols=[f\"{cat_col}_ind\"], outputCols=[f\"{cat_col}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"quantity\",\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ohe = [f\"{cat_col}_ohe\" for cat_col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cat_cols_ohe + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages += [assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build X Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+-------+------+\n",
      "|side|quantity|price|symbol|account|userid|\n",
      "+----+--------+-----+------+-------+------+\n",
      "|SELL|    4862|  443|   ZVV| XYZ789|User_6|\n",
      "|SELL|     933|  190| ZTEST| ABC123|User_4|\n",
      "|SELL|    1127|  136| ZVZZT| XYZ789|User_8|\n",
      "|SELL|    4565|  993| ZWZZT| LMN456|User_7|\n",
      "| BUY|    3645|  299| ZWZZT| ABC123|User_6|\n",
      "+----+--------+-----+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+-------+------+----------+-------------+-----------+-------------+----------+-------------+--------------------+\n",
      "|side|quantity|price|symbol|account|userid|symbol_ind|   symbol_ohe|account_ind|  account_ohe|userid_ind|   userid_ohe|            features|\n",
      "+----+--------+-----+------+-------+------+----------+-------------+-----------+-------------+----------+-------------+--------------------+\n",
      "|SELL|    4862|  443|   ZVV| XYZ789|User_6|       0.0|(6,[0],[1.0])|        0.0|(2,[0],[1.0])|       0.0|(8,[0],[1.0])|(18,[0,6,8,16,17]...|\n",
      "|SELL|     933|  190| ZTEST| ABC123|User_4|       2.0|(6,[2],[1.0])|        2.0|    (2,[],[])|       3.0|(8,[3],[1.0])|(18,[2,11,16,17],...|\n",
      "|SELL|    1127|  136| ZVZZT| XYZ789|User_8|       5.0|(6,[5],[1.0])|        0.0|(2,[0],[1.0])|       1.0|(8,[1],[1.0])|(18,[5,6,9,16,17]...|\n",
      "|SELL|    4565|  993| ZWZZT| LMN456|User_7|       4.0|(6,[4],[1.0])|        1.0|(2,[1],[1.0])|       5.0|(8,[5],[1.0])|(18,[4,7,13,16,17...|\n",
      "| BUY|    3645|  299| ZWZZT| ABC123|User_6|       4.0|(6,[4],[1.0])|        2.0|    (2,[],[])|       0.0|(8,[0],[1.0])|(18,[4,8,16,17],[...|\n",
      "+----+--------+-----+------+-------+------+----------+-------------+-----------+-------------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.save('../models/pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['features','side'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|side|\n",
      "+--------------------+----+\n",
      "|(18,[0,6,8,16,17]...|SELL|\n",
      "|(18,[2,11,16,17],...|SELL|\n",
      "|(18,[5,6,9,16,17]...|SELL|\n",
      "|(18,[4,7,13,16,17...|SELL|\n",
      "|(18,[4,8,16,17],[...| BUY|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Fearture Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "\n",
      "DenseMatrix([[ 1.00000000e+00, -1.68552011e-01, -1.67671849e-01,\n",
      "              -1.67643218e-01, -1.67452295e-01, -1.67379083e-01,\n",
      "               7.21497342e-04, -3.89545087e-05, -2.40880835e-03,\n",
      "              -1.05990203e-03,  1.41706853e-03, -3.10081123e-03,\n",
      "               2.58141142e-03,  3.51857396e-03, -7.98421301e-04,\n",
      "              -1.08290849e-03, -3.63374269e-03, -1.40528436e-04],\n",
      "             [-1.68552011e-01,  1.00000000e+00, -1.67300279e-01,\n",
      "              -1.67271712e-01, -1.67081212e-01, -1.67008162e-01,\n",
      "               1.99464793e-03, -1.15196072e-03, -4.33009692e-04,\n",
      "               1.29668516e-03,  1.42567289e-03,  7.67284997e-04,\n",
      "              -1.48993177e-03, -3.53923323e-03,  2.65736573e-03,\n",
      "               2.24349099e-04,  1.50024546e-03,  1.76119999e-03],\n",
      "             [-1.67671849e-01, -1.67300279e-01,  1.00000000e+00,\n",
      "              -1.66398236e-01, -1.66208730e-01, -1.66136062e-01,\n",
      "              -4.46025191e-03,  5.49654333e-03, -1.33632447e-05,\n",
      "               2.18334482e-03,  2.01602017e-03, -4.67978593e-03,\n",
      "              -7.86750208e-04, -4.79279765e-04,  8.78626193e-04,\n",
      "              -2.57798608e-04,  1.43705851e-03,  8.59771088e-04],\n",
      "             [-1.67643218e-01, -1.67271712e-01, -1.66398236e-01,\n",
      "               1.00000000e+00, -1.66180349e-01, -1.66107693e-01,\n",
      "              -1.81981410e-04,  3.16292266e-04,  2.81611330e-04,\n",
      "              -2.19907879e-03, -2.70725114e-03,  6.03813566e-03,\n",
      "               8.17791554e-04, -2.50657134e-03, -9.39571884e-04,\n",
      "               1.05458120e-03,  2.31310498e-03,  7.34349282e-04],\n",
      "             [-1.67452295e-01, -1.67081212e-01, -1.66208730e-01,\n",
      "              -1.66180349e-01,  1.00000000e+00, -1.65918519e-01,\n",
      "               1.00007221e-04, -2.75407597e-04, -2.59483822e-03,\n",
      "               2.67902656e-03,  9.07382267e-04, -9.42733787e-05,\n",
      "              -2.23837107e-03,  5.56402456e-05,  1.11835371e-03,\n",
      "              -2.31745213e-04, -1.52492749e-03, -2.21702023e-03],\n",
      "             [-1.67379083e-01, -1.67008162e-01, -1.66136062e-01,\n",
      "              -1.66107693e-01, -1.65918519e-01,  1.00000000e+00,\n",
      "              -9.80093060e-04, -1.75077622e-03,  1.76917516e-03,\n",
      "              -9.24579623e-04, -5.88572284e-04,  1.02776311e-03,\n",
      "              -1.32778755e-03,  1.17845875e-03, -5.51324532e-04,\n",
      "               1.48675467e-03, -7.76497616e-04,  3.86808825e-04],\n",
      "             [ 7.21497342e-04,  1.99464793e-03, -4.46025191e-03,\n",
      "              -1.81981410e-04,  1.00007221e-04, -9.80093060e-04,\n",
      "               1.00000000e+00, -5.00154871e-01, -1.98334005e-03,\n",
      "               2.02623516e-03, -5.05698981e-04,  4.22034308e-03,\n",
      "               4.75544761e-04, -1.59175255e-03, -2.55488762e-03,\n",
      "               1.33646219e-03, -2.60401059e-04, -2.49696250e-03],\n",
      "             [-3.89545087e-05, -1.15196072e-03,  5.49654333e-03,\n",
      "               3.16292266e-04, -2.75407597e-04, -1.75077622e-03,\n",
      "              -5.00154871e-01,  1.00000000e+00,  5.89324270e-04,\n",
      "               6.87023308e-05,  1.22633438e-03, -4.06393979e-03,\n",
      "              -1.50614804e-05,  2.08422302e-03,  2.78704758e-03,\n",
      "              -5.69120829e-03,  2.44261978e-03,  3.25309913e-03],\n",
      "             [-2.40880835e-03, -4.33009692e-04, -1.33632447e-05,\n",
      "               2.81611330e-04, -2.59483822e-03,  1.76917516e-03,\n",
      "              -1.98334005e-03,  5.89324270e-04,  1.00000000e+00,\n",
      "              -1.25868645e-01, -1.25744918e-01, -1.25656500e-01,\n",
      "              -1.25547404e-01, -1.25520859e-01, -1.25332005e-01,\n",
      "              -1.25101625e-01, -6.18364714e-04,  2.93767578e-03],\n",
      "             [-1.05990203e-03,  1.29668516e-03,  2.18334482e-03,\n",
      "              -2.19907879e-03,  2.67902656e-03, -9.24579623e-04,\n",
      "               2.02623516e-03,  6.87023308e-05, -1.25868645e-01,\n",
      "               1.00000000e+00, -1.25556977e-01, -1.25468691e-01,\n",
      "              -1.25359758e-01, -1.25333253e-01, -1.25144682e-01,\n",
      "              -1.24914646e-01,  3.81104150e-04, -8.91611710e-05],\n",
      "             [ 1.41706853e-03,  1.42567289e-03,  2.01602017e-03,\n",
      "              -2.70725114e-03,  9.07382267e-04, -5.88572284e-04,\n",
      "              -5.05698981e-04,  1.22633438e-03, -1.25744918e-01,\n",
      "              -1.25556977e-01,  1.00000000e+00, -1.25345357e-01,\n",
      "              -1.25236531e-01, -1.25210052e-01, -1.25021666e-01,\n",
      "              -1.24791857e-01, -1.94738093e-03,  1.70520030e-04],\n",
      "             [-3.10081123e-03,  7.67284997e-04, -4.67978593e-03,\n",
      "               6.03813566e-03, -9.42733787e-05,  1.02776311e-03,\n",
      "               4.22034308e-03, -4.06393979e-03, -1.25656500e-01,\n",
      "              -1.25468691e-01, -1.25345357e-01,  1.00000000e+00,\n",
      "              -1.25148471e-01, -1.25122010e-01, -1.24933757e-01,\n",
      "              -1.24704109e-01,  1.82106844e-03, -4.47164365e-03],\n",
      "             [ 2.58141142e-03, -1.48993177e-03, -7.86750208e-04,\n",
      "               8.17791554e-04, -2.23837107e-03, -1.32778755e-03,\n",
      "               4.75544761e-04, -1.50614804e-05, -1.25547404e-01,\n",
      "              -1.25359758e-01, -1.25236531e-01, -1.25148471e-01,\n",
      "               1.00000000e+00, -1.25013378e-01, -1.24825288e-01,\n",
      "              -1.24595840e-01, -3.55538613e-04,  3.13696943e-03],\n",
      "             [ 3.51857396e-03, -3.53923323e-03, -4.79279765e-04,\n",
      "              -2.50657134e-03,  5.56402456e-05,  1.17845875e-03,\n",
      "              -1.59175255e-03,  2.08422302e-03, -1.25520859e-01,\n",
      "              -1.25333253e-01, -1.25210052e-01, -1.25122010e-01,\n",
      "              -1.25013378e-01,  1.00000000e+00, -1.24798896e-01,\n",
      "              -1.24569496e-01, -9.46100686e-04,  1.11242972e-03],\n",
      "             [-7.98421301e-04,  2.65736573e-03,  8.78626193e-04,\n",
      "              -9.39571884e-04,  1.11835371e-03, -5.51324532e-04,\n",
      "              -2.55488762e-03,  2.78704758e-03, -1.25332005e-01,\n",
      "              -1.25144682e-01, -1.25021666e-01, -1.24933757e-01,\n",
      "              -1.24825288e-01, -1.24798896e-01,  1.00000000e+00,\n",
      "              -1.24382074e-01, -9.67828944e-04,  3.22611076e-03],\n",
      "             [-1.08290849e-03,  2.24349099e-04, -2.57798608e-04,\n",
      "               1.05458120e-03, -2.31745213e-04,  1.48675467e-03,\n",
      "               1.33646219e-03, -5.69120829e-03, -1.25101625e-01,\n",
      "              -1.24914646e-01, -1.24791857e-01, -1.24704109e-01,\n",
      "              -1.24595840e-01, -1.24569496e-01, -1.24382074e-01,\n",
      "               1.00000000e+00,  1.80132140e-03,  3.15694749e-03],\n",
      "             [-3.63374269e-03,  1.50024546e-03,  1.43705851e-03,\n",
      "               2.31310498e-03, -1.52492749e-03, -7.76497616e-04,\n",
      "              -2.60401059e-04,  2.44261978e-03, -6.18364714e-04,\n",
      "               3.81104150e-04, -1.94738093e-03,  1.82106844e-03,\n",
      "              -3.55538613e-04, -9.46100686e-04, -9.67828944e-04,\n",
      "               1.80132140e-03,  1.00000000e+00, -1.48345763e-03],\n",
      "             [-1.40528436e-04,  1.76119999e-03,  8.59771088e-04,\n",
      "               7.34349282e-04, -2.21702023e-03,  3.86808825e-04,\n",
      "              -2.49696250e-03,  3.25309913e-03,  2.93767578e-03,\n",
      "              -8.91611710e-05,  1.70520030e-04, -4.47164365e-03,\n",
      "               3.13696943e-03,  1.11242972e-03,  3.22611076e-03,\n",
      "               3.15694749e-03, -1.48345763e-03,  1.00000000e+00]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\")\n",
    "print(r1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale X Features\n",
    "\n",
    "X features need to be scaled as we are using logistic regression for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscalerModel = xscaler.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscalerModel.write().overwrite().save(\"../models/xscaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xscalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+\n",
      "|            features|side|      scaledFeatures|\n",
      "+--------------------+----+--------------------+\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|\n",
      "|(18,[2,11,16,17],...|SELL|(18,[2,11,16,17],...|\n",
      "|(18,[5,6,9,16,17]...|SELL|(18,[5,6,9,16,17]...|\n",
      "|(18,[4,7,13,16,17...|SELL|(18,[4,7,13,16,17...|\n",
      "|(18,[4,8,16,17],[...| BUY|(18,[4,8,16,17],[...|\n",
      "+--------------------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Target Variable\n",
    "The target variable is categorical, and therefore needs to be converted into a binary (1,0) variable. To do this, we use the StringIndexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "yencoder = StringIndexer(inputCol='side', outputCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coder = yencoder.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y_coder.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coder.save('../models/y_coder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.7,0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+------+\n",
      "|            features|side|      scaledFeatures|target|\n",
      "+--------------------+----+--------------------+------+\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "+--------------------+----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+------+\n",
      "|            features|side|      scaledFeatures|target|\n",
      "+--------------------+----+--------------------+------+\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|\n",
      "+--------------------+----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are training the model on random data, we do not expect a solid result. To ensure probabilities are different from each other, I have set the elasticNet param to 0. This is to ensure that no penalty is applied to the weights of the regression output - flattening the results to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0,featuresCol='scaledFeatures', labelCol = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_dt = lrModel.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_dt = lrModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|side|      scaledFeatures|target|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----+--------------------+------+--------------------+--------------------+----------+\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|[-0.0162006184858...|[0.49594993395985...|       1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|[-0.0158202228426...|[0.49604502677636...|       1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|[-0.0186460485016...|[0.49533862292753...|       1.0|\n",
      "|(18,[0,6,8,16,17]...| BUY|(18,[0,6,8,16,17]...|   1.0|[-0.0169210430769...|[0.49576984016249...|       1.0|\n",
      "|(18,[0,6,8,16,17]...|SELL|(18,[0,6,8,16,17]...|   0.0|[-0.0176108557476...|[0.49559739984852...|       1.0|\n",
      "+--------------------+----+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds_dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = test_preds_dt.rdd.map(lambda lp: (lp.prediction,float(lp.target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "As expected, the model performed poorly because we are trying to model random data. The results are essentially a coin flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.5015155950064416\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18048. 14439.]\n",
      " [17823. 14435.]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"../models/lr_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4b - Deploy Model as New Stream\n",
    "\n",
    "Since we have used Kafka for part 2, we decided the new stream should feed back into kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "from pyspark.ml.feature import StringIndexerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "Two functions were created to generate the new stream with predictions.\n",
    "1. `transform_data` converts the stream into the correct format for the machine learning model.\n",
    "2. `generate_spark_stream` predicts the result on the stream and then publishes the stream to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_df):\n",
    "    '''\n",
    "    Function to complete all necessary pre-model data transformations. \n",
    "    @param input_df: Input data\n",
    "    \n",
    "    Returns:\n",
    "    df (DataFrame): spark dataframe.\n",
    "    '''\n",
    "    ## Load Models\n",
    "    pipeline_model = PipelineModel.load('../models/pipeline_model')\n",
    "    xscalerModel = StandardScalerModel.load('../models/xscaler')\n",
    "    y_coder = StringIndexerModel.load('../models/y_coder')\n",
    "    print(\"Models Loaded\")\n",
    "    ## Outline Columns\n",
    "    cat_cols = ['symbol','account','userid']\n",
    "    cols = [\"side\",'quantity','price','symbol','account','userid']\n",
    "    print(\"DataFrame Subset complete\")\n",
    "    ## Transform data\n",
    "    df = pipeline_model.transform(input_df)\n",
    "    print(\"Variable Cleaning Complete\")\n",
    "    df = xscalerModel.transform(df)\n",
    "    print(\"X variables scaled\")\n",
    "    df = y_coder.transform(df)\n",
    "    print(\"Complete\")\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_spark_stream(input_df, cols):\n",
    "    '''\n",
    "    This function predicts the output on the data stream and writes back into Kafka\n",
    "    @params input_df: spark streaming dataframe\n",
    "    @params cols: columns to publish to kafka stream\n",
    "    \n",
    "    '''\n",
    "    ## Load LG Model\n",
    "    lrModel = LogisticRegressionModel.load(\"../models/lr_model\")\n",
    "    df = transform_data(input_df)\n",
    "    df = lrModel.transform(df)\n",
    "    df \\\n",
    "      .select(F.col('symbol').cast(StringType()).alias('key'),F.to_json(F.struct([F.col(c).alias(c) for c in cols])).alias('value')) \\\n",
    "      .writeStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "      .option(\"checkpointLocation\", \"checkpoint/data\") \\\n",
    "      .option(\"topic\", \"prediction_topic\") \\\n",
    "      .start()\n",
    "    print(\"Stream Created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df, raw_stream_df = generate_stocktrades_stream(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'side',\n",
    " 'quantity',\n",
    " 'price',\n",
    " 'symbol',\n",
    " 'account',\n",
    " 'userid',\n",
    " 'probability',\n",
    " 'prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded\n",
      "DataFrame Subset complete\n",
      "Variable Cleaning Complete\n",
      "X variables scaled\n",
      "Complete\n",
      "Stream Created\n"
     ]
    }
   ],
   "source": [
    "generate_spark_stream(raw_stream_df, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktrade_stream_df.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
